<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta
      name="description"
      content="Agent Self-Dialogue allows user input minimization"
    />
    <meta
      name="keywords"
      content="AIUTA, VLM, LLM, Large Language Model, LLama, Vision Language Model, LLava, Embodied AI, Navigation, Uncertainty, Instance Object Navigation, Object Goal Navigation, VLM uncertainty, ICCV 25"
    />
    <meta
      name="google-site-verification"
      content="rvZfDSs9SdTOr4oSUadPVLK7hrmPjfievNepG09VJm8"
    />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>
      Collaborative Instance Object Navigation: Leveraging Uncertainty-Awareness
      to Minimize Human-Agent Dialogues
    </title>

    <link
      href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
      rel="stylesheet"
    />

    <link rel="stylesheet" href="./static/css/bulma.min.css" />
    <!-- <link rel="stylesheet" href="./static/css/bulma-carousel.min.css" />
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css" /> -->
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css" />
    <link
      rel="stylesheet"
      href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css"
    />
    <link rel="stylesheet" href="./static/css/index.css" />
    <link rel="icon" href="./static/images/flaticon_coin_128.png" />

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./static/js/fontawesome.all.min.js"></script>
    <!-- <script src="./static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/bulma-slider.min.js"></script> -->
    <script src="./static/js/index.js"></script>
  </head>
  <body>
    <nav class="navbar" role="navigation" aria-label="main navigation">
      <div class="navbar-brand">
        <a
          role="button"
          class="navbar-burger"
          aria-label="menu"
          aria-expanded="false"
        >
          <span aria-hidden="true"></span>
          <span aria-hidden="true"></span>
          <span aria-hidden="true"></span>
        </a>
      </div>
      <div class="navbar-menu">
        <div class="navbar-start" style="flex-grow: 1; justify-content: center">
          <a class="navbar-item" href="https://francescotaioli.github.io/">
            <span class="icon">
              <i class="fas fa-home"></i>
            </span>
          </a>

          <div class="navbar-item has-dropdown is-hoverable">
            <a class="navbar-link"> More Research </a>
            <div class="navbar-dropdown">
              <a
                class="navbar-item"
                href="https://intelligolabs.github.io/Le-RNR-Map/"
              >
                <b>Language-enhanced RNR-Map</b> (Vision-Language aligned map)
              </a>
              <a
                class="navbar-item"
                href="https://intelligolabs.github.io/R2RIE-CE/"
              >
                <b>Error Detection and Localization in VLN</b>
              </a>
              <a
                class="navbar-item"
                href="https://intelligolabs.github.io/unsupervised_active_visual_search/"
              >
                <b>POMP-BE-PD</b> (POMCP-based Object Goal Nav.)
              </a>
            </div>
          </div>
        </div>
      </div>
    </nav>

    <section class="hero">
      <div class="hero-body" style="padding: 0%">
        <div class="container is-max-desktop">
          <div class="columns is-centered">
            <div class="column has-text-centered">
              <h1 class="title is-1 publication-title">
                Collaborative Instance Object Navigation: Leveraging
                Uncertainty-Awareness to Minimize Human-Agent Dialogues
              </h1>
              <div class="is-size-6 publication-authors">
                <span class="author-block">
                  <a href="https://francescotaioli.github.io/" target="_blank"
                    >Francesco Taioli</a
                  ><sup>1,2</sup>,</span
                >
                <span class="author-block">
                  <a
                    href="https://scholar.google.com/citations?hl=it&user=fqdv3d4AAAAJ&view_op=list_works&sortby=pubdate"
                    target="_blank"
                    >Edoardo Zorzi</a
                  ><sup>2</sup>,
                </span>
                <span class="author-block"
                  ><a
                    href="https://scholar.google.it/citations?hl=it&newwindow=1&user=ZCW6-psAAAAJ&view_op=list_works&sortby=pubdate"
                    target="_blank"
                    >Gianni Franchi</a
                  ><sup>3</sup>,
                </span>
                <span class="author-block">
                  <a
                    href="https://scholar.google.it/citations?hl=it&user=AploBScAAAAJ&view_op=list_works&sortby=pubdate"
                    target="_blank"
                    >Alberto Castellini</a
                  ><sup>2</sup>,
                </span>
                <span class="author-block">
                  <a
                    href="https://scholar.google.co.uk/citations?hl=en&user=KHAIAA8AAAAJ&view_op=list_works&sortby=pubdate"
                    target="_blank"
                    >Alessandro Farinelli</a
                  ><sup>2</sup>,
                </span>
                <span class="author-block"
                  ><a
                    href="https://scholar.google.com/citations?hl=en&user=LbgTPRwAAAAJ&view_op=list_works&sortby=pubdate"
                    target="_blank"
                    >Marco Cristani</a
                  ><sup>2,5</sup>,
                </span>
                <span class="author-block"
                  ><a
                    href="https://scholar.google.co.uk/citations?hl=en&user=KBZ3zrEAAAAJ&view_op=list_works&sortby=pubdate"
                    target="_blank"
                    >Yiming Wang</a
                  ><sup>4</sup>
                </span>
              </div>

              <div class="is-size-6 publication-authors">
                <span class="author-block"
                  ><sup>1</sup>Polytechnic of Turin,</span
                >
                <span class="author-block"
                  ><sup>2</sup>University of Verona,</span
                >
                <span class="author-block"><sup>3</sup>U2IS, ENSTA Paris</span>
                <span class="author-block"
                  ><sup>4</sup>Fondazione Bruno Kessler</span
                >
                <span class="author-block"
                  ><sup>5</sup>University of Reykjavik</span
                >
              </div>

              <div class="column has-text-centered">
                <div class="publication-links">
                  <span class="link-block">
                    <span
                      class="tag is-success is-rounded is-large is-light"
                      style="
                        border: 2px solid rgb(45, 108, 54);
                        color: rgb(45, 108, 54);
                      "
                      >ICCV 25🎉</span
                    >
                  </span>
                  <span class="has-text-grey mx-2" style="font-weight: bold"
                    >|</span
                  >

                  <span class="link-block">
                    <a
                      href="https://arxiv.org/abs/2412.01250"
                      class="button_link"
                      target="_blank"
                    >
                      <span class="icon is-small">
                        <i class="ai ai-arxiv"></i>
                      </span>
                      arXiv
                    </a>
                    <a
                      href="https://github.com/intelligolabs/CoIN"
                      class="button_link"
                      target="_blank"
                    >
                      <span class="icon is-small">
                        <i class="fab fa-github"></i>
                      </span>
                      <span>Code</span>
                    </a>
                  </span>
                  <span class="link-block">
                    <a
                      href="https://huggingface.co/datasets/ftaioli/CoIN-Bench"
                      class="button_link"
                      target="_blank"
                    >
                      <span class="icon is-small">
                        <img
                          src="static/images/huggingface_logo.svg"
                          alt="Hugging Face logo"
                          style="height: 1em"
                        />
                      </span>
                      <span>CoIN-Bench</span>
                    </a>
                  </span>
                  <span class="link-block">
                    <a
                      href="https://huggingface.co/datasets/ftaioli/IDKVQA"
                      class="button_link"
                      target="_blank"
                    >
                      <span class="icon is-small">
                        <img
                          src="static/images/huggingface_logo.svg"
                          alt="Hugging Face logo"
                          style="height: 1em"
                        />
                      </span>
                      <span>IDKVQA dataset</span>
                    </a>
                  </span>
                </div>
              </div>

              <div class="box has-text-left">
                <article class="media">
                  <figure class="media-left">
                    <img
                      src="static/images/error.svg"
                      style="width: 48px; padding-top: 50%"
                    />
                  </figure>
                  <div class="media-content">
                    <p>
                      <strong>Hallucination reduction:</strong> We introduce a
                      novel <em>Normalized-Entropy-based</em> technique to
                      quantify VLM perception uncertainty, along with a
                      dedicated VQA embodied dataset (<strong>IDKVQA</strong>),
                      in which responses can be <em>Yes</em>, <em>No</em>, or
                      <em>I Don't Know</em>. See the
                      <a
                        href="https://github.com/intelligolabs/CoIN/tree/master/notebook"
                        target="_blank"
                        >python notebook</a
                      >
                      and the
                      <a href="https://huggingface.co/datasets/ftaioli/IDKVQA"
                        >IDKVQA dataset</a
                      >.
                    </p>
                  </div>
                </article>

                <article class="media">
                  <figure class="media-left">
                    <img
                      src="static/images/human.svg"
                      style="width: 48px; padding-top: 50%"
                    />
                  </figure>
                  <div class="media-content">
                    <p>
                      <strong>Natural Human-Robot Interaction:</strong>
                      CoIN-Bench introduces, for the first time,
                      <em>template-free, open-ended, bidirectional</em>
                      human &harr; agent dialogues. We simulate user responses
                      via a VLM with access to high-resolution images of target
                      objects. See the <a href="#Video">video</a> for an example
                      of a simulated interaction.
                    </p>
                  </div>
                </article>

                <article class="media">
                  <figure class="media-left">
                    <img
                      src="static/images/thinking-svgrepo-com.svg"
                      style="width: 48px; padding-top: 50%"
                    />
                  </figure>
                  <div class="media-content">
                    <p>
                      <strong>Self-Dialogue:</strong> Our method, AIUTA,
                      operates in a zero-shot manner, generalizing across scenes
                      and objects. Before taking any actions, AIUTA engages in a
                      Self-Dialogue between an on-board LLM and a VLM: the
                      <em>Self-Questioner</em> module effectively reduces
                      hallucinations and gather more environmental information,
                      while the <em>Interaction Trigger</em> minimizes
                      agent-user interactions. See the
                      <a href="#Self-Questioner">Self-Questioner figure</a> for
                      an example!
                    </p>
                  </div>
                </article>
              </div>
              <hr />
            </div>
          </div>
        </div>
      </div>
    </section>

    <section class="hero teaser">
      <div class="container">
        <div class="columns is-centered">
          <div class="column is-max-desktop">
            <div class="content has-text-centered">
              <div class="columns is-centered">
                <div class="column is-10">
                  <h2 class="title is-3">Motivation</h2>
                  <div class="content has-text-justified">
                    <!-- content -->
                    <section
                      class="section"
                      style="padding-top: 0px !important"
                    >
                      <div class="container">
                        <p class="is-italic mb-4">
                          <strong>The ObjectGoal Nav task</strong> is
                          <em>under</em>-specified: the input is a
                          <i>category c</i>, and the goal is to find
                          <i>any</i> instance of that category. As we show
                          below, which one should we navigate to?
                        </p>

                        <figure
                          class="image is-inline-block"
                          style="margin-top: 0em"
                        >
                          <img
                            src="static/images/picture.png"
                            alt="ObjectGoal example"
                            style="max-width: 100%; height: auto"
                          />
                        </figure>

                        <!-- <p class="mt-4 is-size-5 has-text-weight-semibold">
                          Which one?
                        </p> -->

                        <p class="is-italic mb-4">
                          On the other hand, the
                          <strong>InstanceObjectGoal Nav task</strong>
                          is <em>often</em> <b>ambiguous</b> and
                          <b><em>over</em>-specified:</b>
                        </p>

                        <figure
                          class="image is-inline-block"
                          style="margin-top: 0em"
                        >
                          <img
                            src="static/images/bed.png"
                            alt="InstanceObjectGoal example"
                            style="max-width: 100%; height: auto"
                          />
                        </figure>

                        <p class="mt-4 is-italic">
                          The description
                          <i
                            >“A bed with cushions and a colored comforter, two
                            lamps, and a carpet on the floor”</i
                          >
                          could apply to all the beds shown in the image above.
                        </p>
                        <p>Furthermore,</p>

                        <ol>
                          <li>
                            humans aim to <b>minimum input</b>: providing a
                            long, detailed description before navigation is
                            demanding and time-consuming.
                            <i>Would you want to give such a description?</i>
                          </li>
                          <li>
                            <b
                              >Agents should ask clarifying questions if
                              necessary.</b
                            >
                          </li>
                        </ol>
                        <!-- </p> -->

                        <!-- Vertical arrow -->

                        <div
                          class="my-4 has-text-centered"
                          style="
                            padding-top: 0px !important;
                            margin-bottom: 0px !important;
                          "
                        >
                          <span style="font-size: 2rem">⬇️</span>
                        </div>
                      </div>
                    </section>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>

    <section class="hero teaser">
      <div class="container">
        <div class="columns is-centered">
          <div class="column is-max-desktop">
            <div class="content has-text-centered">
              <div class="columns is-centered">
                <div class="column is-10">
                  <h2 class="title is-3">AIUTA: Our Method</h2>
                  <div class="content has-text-justified">
                    <!-- content -->
                    <section
                      class="section"
                      style="padding-top: 0px !important"
                    >
                      <div class="container">
                        <p>
                          We propose a novel <b>embodied reasoning</b> method
                          for human-agent interaction reasoning. Our method
                          integrates two key components: a
                          <strong>Self-Questioner</strong> and an
                          <strong>Interaction Trigger</strong>.
                        </p>

                        <p>Specifically, the agent:</p>

                        <ol>
                          <li>
                            <p>
                              Executes a <b>self-questioning</b> procedure to
                              automatically gather more information from the
                              environment (<i>e.g., Is the whall light blue?</i
                              >), reduce hallucination through a novel
                              normalized-entropy-based technique, and generate a
                              refined description of its observations.
                            </p>
                            <p>
                              Our agent is equipped with a Large Language Model
                              (LLM) and a Vision-Language Model (VLM). When
                              environmental information is missing, the
                              self-questioning mechanism is designed from the
                              ground up to automatically leverage the LLM to
                              generate the most informative question, and then
                              use the VLM to extract the corresponding answer
                              from the visual input.
                            </p>
                          </li>
                          <li>
                            Decides whether to ask a clarifying question to the
                            user, continue navigating, or halt exploration,
                            based on the refined description and the “facts”
                            associated with the target.
                          </li>
                        </ol>

                        <p class="mt-4">
                          See the illustration below, based on a real example of
                          our method in action.
                        </p>
                      </div>
                    </section>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>

    <section class="hero teaser" id="Self-Questioner">
      <div class="container">
        <div class="columns is-centered">
          <div class="column is-max-desktop">
            <div class="content has-text-centered">
              <div class="columns is-centered">
                <div class="column is-10">
                  <img
                    class="image is-fullwidth"
                    src="./static/images/teaser.png"
                    alt="Teaser"
                  />
                  <div class="content has-text-justified">
                    <p style="padding: 1em 2em 0em 2em">
                      Sketched episode of the proposed
                      <b><i>Collaborative Instance Navigation (CoIN)</i></b>
                      task. The human user (bottom left) provides a request (<i
                        >"Find the picture"</i
                      >
                      ) in <i>natural language</i>. The agent has to locate the
                      object within a <i>completely unknown environment</i>,
                      interacting with the user only when needed via
                      <i>template-free, open-ended</i> natural-language
                      dialogue. Our method, <b>A</b>gent-user <b>I</b>nteraction
                      with <b>U</b>ncerTainty <b>A</b>wareness (<b>AIUTA</b>),
                      addresses this challenging task, minimizing user
                      interactions by equipping the agent with two modules: a
                      <b>Self-Questioner</b> and an <b>Interaction Trigger</b>,
                      whose output is shown in the blue boxes along the agent’s
                      path (① to ⑤), and whose inner working is shown on the
                      right. The <b>Self-Questioner</b> leverages a Large
                      Language Model (LLM) and Vision Language Model (VLM) in a
                      self-dialogue to initially describe the agent’s
                      observation, and then extract additional relevant details,
                      with a novel entropy-based technique to reduce
                      <b
                        ><font color="red"
                          >hallucinations and inaccuracies</font
                        ></b
                      >, producing a refined
                      <b><font color="green">detection description</font></b
                      >. The <b>Interaction Trigger</b> uses this refined
                      description to decide whether to pose a question to the
                      user (①,③,④), continue the navigation (②) or halt the
                      exploration (⑤).
                    </p>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>

    <section class="hero teaser">
      <div class="container">
        <div class="columns is-centered">
          <div class="column is-max-desktop">
            <div class="content has-text-centered">
              <div class="columns is-centered">
                <div class="column is-10">
                  <br />
                  <h2 class="title is-3">Abstract</h2>
                  <div class="content has-text-justified">
                    <p style="padding: 0em 2em 0em 2em">
                      Language-driven instance object navigation assumes that
                      human users initiate the task by providing a detailed
                      description of the target instance to the embodied agent.
                      While this description is crucial for distinguishing the
                      target from visually similar instances in a scene,
                      providing it prior to navigation can be demanding for
                      human.
                    </p>

                    <p style="padding: 0em 2em 0em 2em">
                      To bridge this gap, we introduce
                      <b>Collaborative Instance object Navigation (CoIN)</b>, a
                      new task setting where the agent actively resolve
                      uncertainties about the target instance during navigation
                      in natural, template-free, open-ended dialogues with
                      human.
                    </p>
                    <p style="padding: 0em 2em 0em 2em">
                      We propose a novel training-free method,
                      <b
                        >Agent user Interaction with UncerTainty Awareness
                        (AIUTA)</b
                      >, which operates independently from the navigation
                      policy, and focuses on the human-agent interaction
                      reasoning with Vision-Language Models (VLMs) and Large
                      Language Models (LLMs). First, upon object detection, a
                      <b>Self-Questioner</b>
                      model initiates a self-dialogue within the agent to obtain
                      a complete and accurate observation description with a
                      novel uncertainty estimation technique. Then, an
                      <b>Interaction Trigger</b> module determines whether to
                      ask a question to the human, continue or halt navigation,
                      minimizing user input.
                    </p>

                    <p style="padding: 0em 2em 0em 2em">
                      For evaluation, we introduce
                      <b>CoIN-Bench</b>, with a curated dataset designed for
                      challenging multi-instance scenarios. CoIN-Bench supports
                      both online evaluation with humans and reproducible
                      experiments with simulated user-agent interactions. On
                      CoIN-Bench, we show that AIUTA serves as a competitive
                      baseline, while existing language-driven instance
                      navigation methods struggle in complex multi-instance
                      scenes.
                    </p>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>

    <section class="section">
      <div class="container is-max-desktop is-centered">
        <!-- Paper video. -->
        <div class="columns is-centered has-text-centered">
          <div class="column is-is-full">
            <h2 class="title is-3">Method</h2>
            <img
              class="image is-fullwidth"
              src="./static/images/arch_1.png"
              alt="Teaser"
            />

            <br />
            <p style="text-align: justify">
              Graphical depiction of <b>AIUTA</b>: left shows its interaction
              cycle with the user, and right provides an exploded view of our
              method. ① The agent receives an initial instruction <i>I</i>:
              "Find a c = <i>< object category > </i>". ② At each timestep t, a
              zero-shot policy π (VLFM), comprising a frozen object detection
              module, selects the optimal action a<sub>t</sub>. ③ Upon
              detection, the agent performs the proposed AIUTA. Specifically, ④
              the agent first obtains an initial scene description of
              observation O<sub>t</sub> from a VLM. Then, a Self-Questioner
              module leverages an LLM to automatically generate
              attribute-specific questions to the VLM, acquiring more
              information and refining the scene description with reduced
              attribute-level uncertainty, producing S<sub>refined</sub>. ⑤ The
              Interaction Trigger module then evaluates S<sub>refined</sub>
              against the “facts” related to the target, to determine whether to
              terminate the navigation (if the agent believes it has located the
              target object ⑥), or to pose template-free, natural-language
              questions to a human ⑦, updating the “facts” based on the response
              ⑧.
            </p>
            <hr />
          </div>
        </div>
        <!--/ Paper video. -->
      </div>
    </section>

    <section class="hero teaser">
      <div class="container">
        <div class="columns is-centered">
          <div class="column is-max-desktop">
            <div class="content has-text-centered">
              <div class="columns is-centered">
                <div class="column is-12">
                  <h2 class="title is-4">
                    <span class="link-block">
                      <a
                        href="https://huggingface.co/datasets/ftaioli/IDKVQA"
                        class="button_link"
                        target="_blank"
                      >
                        <span class="is-small">
                          <img
                            src="static/images/huggingface_logo.svg"
                            alt="Hugging Face logo"
                            style="height: 1em; width: auto"
                          />
                        </span>
                        <span>I don't know VQA dataset - IDKVQA</span>
                      </a>
                    </span>
                  </h2>
                  <div class="content has-text-justified is-centered">
                    <!-- content -->
                    <section
                      class="section"
                      style="padding-top: 0px !important"
                    >
                      <div class="container">
                        <p>
                          <b>Not every question has an answer.</b> That's why we
                          created the I Don't Know VQA dataset, a VQA dataset
                          where answers can be Yes, No, or I don't know.
                        </p>
                        <p>We extensively described the IDKVQA dataset in the supplementary material and in the <a href="https://huggingface.co/datasets/ftaioli/IDKVQA"> HF dataset page.</a></p>

                        <figure
                          class="image is-inline-block"
                          style="margin-top: 0em"
                        >
                          <img
                            src="static/images/idkvqa.png"
                            alt="ObjectGoal example"
                            style="
                              max-width: 50%;
                              height: auto;
                              margin-left: 15%;
                            "
                          />
                        </figure>
                      </div>
                    </section>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>

    <section class="section" id="Video">
      <div class="container is-max-desktop is-centered">
        <!-- Paper video. -->
        <div class="columns is-centered has-text-centered">
          <div class="column is-is-full">
            <h1 class="title is-3">Video</h1>
            <div class="publication-video">
              <iframe
                style="display: block; background-color: white"
                src="static/videos/aiuta_demo.mp4"
                frameborder="0"
                width="1920"
                height="1080"
                allow="autoplay; encrypted-media"
                allowfullscreen
              ></iframe>
            </div>
          </div>
        </div>
        <!--/ Paper video. -->
      </div>
    </section>
    <section class="section" id="BibTeX">
      <div class="container is-max-desktop content">
        <h2 class="title">BibTeX</h2>
        <pre>      <code>
        
        @InProceedings{Taioli_2025_ICCV,
          author    = {Taioli, Francesco and Zorzi, Edoardo and Franchi, Gianni and Castellini, Alberto and Farinelli, Alessandro and Cristani, Marco and Wang, Yiming},
          title     = {Collaborative Instance Object Navigation: Leveraging Uncertainty-Awareness to Minimize Human-Agent Dialogues},
          booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
          month     = {October},
          year      = {2025},
          pages     = {18781-18792}
      }

</code>
</pre>
      </div>
    </section>

    <footer class="footer">
      <div class="container">
        <div class="content has-text-centered"></div>
        <div class="columns is-centered">
          <div class="column is-8">
            <div class="content">
              <p>
                This website is licensed under a
                <a
                  rel="license"
                  href="http://creativecommons.org/licenses/by-sa/4.0/"
                  >Creative Commons Attribution-ShareAlike 4.0 International
                  License</a
                >.
              </p>
              <p>
                This means you are free to borrow the
                <a href="https://github.com/nerfies/nerfies.github.io"
                  >source code</a
                >
                of this website, we just ask that you link back to this page in
                the footer. Please remember to remove the analytics code
                included in the header of the website which you do not want on
                your website.
              </p>
            </div>
          </div>
        </div>
      </div>
    </footer>
  </body>
</html>
