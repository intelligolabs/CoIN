<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta
      name="description"
      content="Agent Self-Dialogue allows user input minimization"
    />
    <meta
      name="keywords"
      content="AIUTA, VLM, LLM, Large Language Model, LLama, Vision Language Model, LLava, Embodied AI, Navigation, Uncertainty, Instance Object Navigation, Object Goal Navigation, VLM uncertainty, ICCV 25"
    />
    <meta
      name="google-site-verification"
      content="rvZfDSs9SdTOr4oSUadPVLK7hrmPjfievNepG09VJm8"
    />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>
      Collaborative Instance Object Navigation: Leveraging Uncertainty-Awareness
      to Minimize Human-Agent Dialogues
    </title>

    <link
      href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
      rel="stylesheet"
    />

    <link rel="stylesheet" href="./static/css/bulma.min.css" />
    <!-- <link rel="stylesheet" href="./static/css/bulma-carousel.min.css" />
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css" /> -->
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css" />
    <link
      rel="stylesheet"
      href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css"
    />
    <link rel="stylesheet" href="./static/css/index.css" />
    <link rel="icon" href="./static/images/flaticon_coin_128.png" />

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./static/js/fontawesome.all.min.js"></script>
    <!-- <script src="./static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/bulma-slider.min.js"></script> -->
    <script src="./static/js/index.js"></script>
  </head>
  <body>
    <nav class="navbar" role="navigation" aria-label="main navigation">
      <div class="navbar-brand">
        <a
          role="button"
          class="navbar-burger"
          aria-label="menu"
          aria-expanded="false"
        >
          <span aria-hidden="true"></span>
          <span aria-hidden="true"></span>
          <span aria-hidden="true"></span>
        </a>
      </div>
      <div class="navbar-menu">
        <div class="navbar-start" style="flex-grow: 1; justify-content: center">
          <a class="navbar-item" href="https://francescotaioli.github.io/">
            <span class="icon">
              <i class="fas fa-home"></i>
            </span>
          </a>

          <div class="navbar-item has-dropdown is-hoverable">
            <a class="navbar-link"> More Research </a>
            <div class="navbar-dropdown">
              <a
                class="navbar-item"
                href="https://intelligolabs.github.io/Le-RNR-Map/"
              >
                Language-enhanced RNR-Map
              </a>
              <a
                class="navbar-item"
                href="https://intelligolabs.github.io/R2RIE-CE/"
              >
                Mind the Error in VLN
              </a>
              <a
                class="navbar-item"
                href="https://intelligolabs.github.io/unsupervised_active_visual_search/"
              >
                POMP-BE-PD
              </a>
            </div>
          </div>
        </div>
      </div>
    </nav>

    <section class="hero">
      <div class="hero-body" style="padding: 0%">
        <div class="container is-max-desktop">
          <div class="columns is-centered">
            <div class="column has-text-centered">
              <h1 class="title is-1 publication-title">
                Collaborative Instance Object Navigation: Leveraging
                Uncertainty-Awareness to Minimize Human-Agent Dialogues
              </h1>
              <div class="is-size-6 publication-authors">
                <span class="author-block">
                  <a href="https://francescotaioli.github.io/" target="_blank"
                    >Francesco Taioli</a
                  ><sup>1,2</sup>,</span
                >
                <span class="author-block">
                  <a
                    href="https://scholar.google.com/citations?hl=it&user=fqdv3d4AAAAJ&view_op=list_works&sortby=pubdate"
                    target="_blank"
                    >Edoardo Zorzi</a
                  ><sup>2</sup>,
                </span>
                <span class="author-block"
                  ><a
                    href="https://scholar.google.it/citations?hl=it&newwindow=1&user=ZCW6-psAAAAJ&view_op=list_works&sortby=pubdate"
                    target="_blank"
                    >Gianni Franchi</a
                  ><sup>3</sup>,
                </span>
                <span class="author-block">
                  <a
                    href="https://scholar.google.it/citations?hl=it&user=AploBScAAAAJ&view_op=list_works&sortby=pubdate"
                    target="_blank"
                    >Alberto Castellini</a
                  ><sup>2</sup>,
                </span>
                <span class="author-block">
                  <a
                    href="https://scholar.google.co.uk/citations?hl=en&user=KHAIAA8AAAAJ&view_op=list_works&sortby=pubdate"
                    target="_blank"
                    >Alessandro Farinelli</a
                  ><sup>2</sup>,
                </span>
                <span class="author-block"
                  ><a
                    href="https://scholar.google.com/citations?hl=en&user=LbgTPRwAAAAJ&view_op=list_works&sortby=pubdate"
                    target="_blank"
                    >Marco Cristani</a
                  ><sup>2</sup>,
                </span>
                <span class="author-block"
                  ><a
                    href="https://scholar.google.co.uk/citations?hl=en&user=KBZ3zrEAAAAJ&view_op=list_works&sortby=pubdate"
                    target="_blank"
                    >Yiming Wang</a
                  ><sup>4</sup>
                </span>
              </div>

              <div class="is-size-6 publication-authors">
                <span class="author-block"
                  ><sup>1</sup>Polytechnic of Turin,</span
                >
                <span class="author-block"
                  ><sup>2</sup>University of Verona,</span
                >
                <span class="author-block"
                  ><sup>3</sup>U2IS, ENSTA Paris, Institut Polytechnique de
                  Paris</span
                >
                <span class="author-block"
                  ><sup>4</sup>Fondazione Bruno Kessler</span
                >
              </div>

              <div class="column has-text-centered">
                <div class="publication-links">
                  <span class="link-block">
                    <span
                      class="tag is-success is-rounded is-large is-light"
                      style="
                        border: 2px solid rgb(45, 108, 54);
                        color: rgb(45, 108, 54);
                      "
                      >ICCV 25🎉</span
                    >
                  </span>
                  <span class="has-text-grey mx-2" style="font-weight: bold"
                    >|</span
                  >

                  <span class="link-block">
                    <a
                      href="https://arxiv.org/abs/2412.01250"
                      class="button_link"
                      target="_blank"
                    >
                      <span class="icon is-small">
                        <i class="ai ai-arxiv"></i>
                      </span>
                      arXiv
                    </a>
                    <a
                      href="https://github.com/intelligolabs/CoIN"
                      class="button_link"
                      target="_blank"
                    >
                      <span class="icon is-small">
                        <i class="fab fa-github"></i>
                      </span>
                      <span>Code</span>
                    </a>
                  </span>
                  <span class="link-block">
                    <a
                      href="https://huggingface.co/datasets/ftaioli/CoIN-Bench"
                      class="button_link"
                      target="_blank"
                    >
                      <span class="icon is-small">
                        <img
                          src="static/images/huggingface_logo.svg"
                          alt="Hugging Face logo"
                          style="height: 1em"
                        />
                      </span>
                      <span>CoIN-Bench</span>
                    </a>
                  </span>
                  <span class="link-block">
                    <a
                      href="https://huggingface.co/datasets/ftaioli/IDKVQA"
                      class="button_link"
                      target="_blank"
                    >
                      <span class="icon is-small">
                        <img
                          src="static/images/huggingface_logo.svg"
                          alt="Hugging Face logo"
                          style="height: 1em"
                        />
                      </span>
                      <span>IDKVQA dataset</span>
                    </a>
                  </span>
                </div>
              </div>

              <div class="box has-text-left">
                <article class="media">
                  <figure class="media-left">
                    <img
                      src="static/images/error.svg"
                      style="width: 48px; padding-top: 50%"
                    />
                  </figure>
                  <div class="media-content">
                    <p>
                      <strong>Hallucination reduction:</strong> We introduce a
                      novel <em>Normalized-Entropy-based</em> technique to
                      quantify VLM perception uncertainty, along with a
                      dedicated VQA embodied dataset (<strong>IDKVQA</strong>),
                      in which responses can be <em>Yes</em>, <em>No</em>, or
                      <em>I Don't Know</em>.
                    </p>
                  </div>
                </article>

                <article class="media">
                  <figure class="media-left">
                    <img
                      src="static/images/human.svg"
                      style="width: 48px; padding-top: 50%"
                    />
                  </figure>
                  <div class="media-content">
                    <p>
                      <strong>Natural Human-Robot Interaction:</strong>
                      CoIN-Bench introduces, for the first time,
                      <em>template-free, open-ended, bidirectional</em>
                      human &harr; agent dialogues. We simulate user responses
                      via a VLM with access to high-resolution images of target
                      objects.
                    </p>
                  </div>
                </article>

                <article class="media">
                  <figure class="media-left">
                    <img
                      src="static/images/thinking-svgrepo-com.svg"
                      style="width: 48px; padding-top: 50%"
                    />
                  </figure>
                  <div class="media-content">
                    <p>
                      <strong>Self-Dialogue:</strong> Our method, AIUTA,
                      operates in a zero-shot manner, generalizing across scenes
                      and objects. Before taking any actions, AIUTA engages in a
                      Self-Dialogue between an on-board LLM and a VLM: the
                      <em>Self-Questioner</em> module effectively reduces
                      hallucinations and gather more environmental information,
                      while the <em>Interaction Trigger</em> minimizes
                      agent-user interactions.
                    </p>
                  </div>
                </article>
              </div>
              <hr />
            </div>
          </div>
        </div>
      </div>
    </section>

    <section class="hero teaser">
      <div class="container">
        <div class="columns is-centered">
          <div class="column is-max-desktop">
            <div class="content has-text-centered">
              <div class="columns is-centered">
                <div class="column is-10">
                  <img
                    class="image is-fullwidth"
                    src="./static/images/teaser.png"
                    alt="Teaser"
                  />
                  <div class="content has-text-justified">
                    <p style="padding: 1em 2em 0em 2em">
                      Sketched episode of the proposed
                      <b><i>Collaborative Instance Navigation (CoIN)</i></b>
                      task. The human user (bottom left) provides a request (<i
                        >"Find the picture"</i
                      >
                      ) in <i>natural language</i>. The agent has to locate the
                      object within a <i>completely unknown environment</i>,
                      interacting with the user only when needed via
                      <i>template-free, open-ended</i> natural-language
                      dialogue. Our method, <b>A</b>gent-user <b>I</b>nteraction
                      with <b>U</b>ncerTainty <b>A</b>wareness (<b>AIUTA</b>),
                      addresses this challenging task, minimizing user
                      interactions by equipping the agent with two modules: a
                      <b>Self-Questioner</b> and an <b>Interaction Trigger</b>,
                      whose output is shown in the blue boxes along the agent’s
                      path (① to ⑤), and whose inner working is shown on the
                      right. The <b>Self-Questioner</b> leverages a Large
                      Language Model (LLM) and Vision Language Model (VLM) in a
                      self-dialogue to initially describe the agent’s
                      observation, and then extract additional relevant details,
                      with a novel entropy-based technique to reduce
                      <b
                        ><font color="red"
                          >hallucinations and inaccuracies</font
                        ></b
                      >, producing a refined
                      <b><font color="green">detection description</font></b
                      >. The <b>Interaction Trigger</b> uses this refined
                      description to decide whether to pose a question to the
                      user (①,③,④), continue the navigation (②) or halt the
                      exploration (⑤).
                    </p>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>

    <section class="hero teaser">
      <div class="container">
        <div class="columns is-centered">
          <div class="column is-max-desktop">
            <div class="content has-text-centered">
              <div class="columns is-centered">
                <div class="column is-10">
                  <br />
                  <h2 class="title is-3">Abstract</h2>
                  <div class="content has-text-justified">
                    <p style="padding: 0em 2em 0em 2em">
                      Language-driven instance object navigation assumes that
                      human users initiate the task by providing a detailed
                      description of the target instance to the embodied agent.
                      While this description is crucial for distinguishing the
                      target from visually similar instances in a scene,
                      providing it prior to navigation can be demanding for
                      human.
                    </p>

                    <p style="padding: 0em 2em 0em 2em">
                      To bridge this gap, we introduce
                      <b>Collaborative Instance object Navigation (CoIN)</b>, a
                      new task setting where the agent actively resolve
                      uncertainties about the target instance during navigation
                      in natural, template-free, open-ended dialogues with
                      human.
                    </p>
                    <p style="padding: 0em 2em 0em 2em">
                      We propose a novel training-free method,
                      <b
                        >Agent user Interaction with UncerTainty Awareness
                        (AIUTA)</b
                      >, which operates independently from the navigation
                      policy, and focuses on the human-agent interaction
                      reasoning with Vision-Language Models (VLMs) and Large
                      Language Models (LLMs). First, upon object detection, a
                      <b>Self-Questioner</b>
                      model initiates a self-dialogue within the agent to obtain
                      a complete and accurate observation description with a
                      novel uncertainty estimation technique. Then, an
                      <b>Interaction Trigger</b> module determines whether to
                      ask a question to the human, continue or halt navigation,
                      minimizing user input.
                    </p>

                    <p style="padding: 0em 2em 0em 2em">
                      For evaluation, we introduce
                      <b>CoIN-Bench</b>, with a curated dataset designed for
                      challenging multi-instance scenarios. CoIN-Bench supports
                      both online evaluation with humans and reproducible
                      experiments with simulated user-agent interactions. On
                      CoIN-Bench, we show that AIUTA serves as a competitive
                      baseline, while existing language-driven instance
                      navigation methods struggle in complex multi-instance
                      scenes.
                    </p>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>

    <section class="section">
      <div class="container is-max-desktop is-centered">
        <!-- Paper video. -->
        <div class="columns is-centered has-text-centered">
          <div class="column is-is-full">
            <h2 class="title is-3">Method</h2>
            <img
              class="image is-fullwidth"
              src="./static/images/arch_1.png"
              alt="Teaser"
            />

            <br />
            <p style="text-align: justify">
              Graphical depiction of <b>AIUTA</b>: left shows its interaction
              cycle with the user, and right provides an exploded view of our
              method. ① The agent receives an initial instruction <i>I</i>:
              "Find a c = <i>< object category > </i>". ② At each timestep t, a
              zero-shot policy π (VLFM), comprising a frozen object detection
              module, selects the optimal action a<sub>t</sub>. ③ Upon
              detection, the agent performs the proposed AIUTA. Specifically, ④
              the agent first obtains an initial scene description of
              observation O<sub>t</sub> from a VLM. Then, a Self-Questioner
              module leverages an LLM to automatically generate
              attribute-specific questions to the VLM, acquiring more
              information and refining the scene description with reduced
              attribute-level uncertainty, producing S<sub>refined</sub>. ⑤ The
              Interaction Trigger module then evaluates S<sub>refined</sub>
              against the “facts” related to the target, to determine whether to
              terminate the navigation (if the agent believes it has located the
              target object ⑥), or to pose template-free, natural-language
              questions to a human ⑦, updating the “facts” based on the response
              ⑧.
            </p>
            <hr />
            <h1 class="title is-3">Video</h1>
            <div class="publication-video">
              <iframe
                style="display: block; background-color: white"
                src="static/videos/aiuta_demo.mp4"
                frameborder="0"
                width="1920"
                height="1080"
                allow="autoplay; encrypted-media"
                allowfullscreen
              ></iframe>
            </div>
          </div>
        </div>
        <!--/ Paper video. -->
      </div>
    </section>
    <section class="section" id="BibTeX">
      <div class="container is-max-desktop content">
        <h2 class="title">BibTeX</h2>
        <pre>      <code>
        
        @misc{taioli2025coin,
            title={{Collaborative Instance Object Navigation: Leveraging Uncertainty-Awareness to Minimize Human-Agent Dialogues}}, 
            author={Francesco Taioli and Edoardo Zorzi and Gianni Franchi and Alberto Castellini and Alessandro Farinelli and Marco Cristani and Yiming Wang},
            year={2025},
            eprint={2412.01250},
            archivePrefix={arXiv},
            primaryClass={cs.AI},
            url={https://arxiv.org/abs/2412.01250}, 
      }

</code>
</pre>
      </div>
    </section>

    <footer class="footer">
      <div class="container">
        <div class="content has-text-centered"></div>
        <div class="columns is-centered">
          <div class="column is-8">
            <div class="content">
              <p>
                This website is licensed under a
                <a
                  rel="license"
                  href="http://creativecommons.org/licenses/by-sa/4.0/"
                  >Creative Commons Attribution-ShareAlike 4.0 International
                  License</a
                >.
              </p>
              <p>
                This means you are free to borrow the
                <a href="https://github.com/nerfies/nerfies.github.io"
                  >source code</a
                >
                of this website, we just ask that you link back to this page in
                the footer. Please remember to remove the analytics code
                included in the header of the website which you do not want on
                your website.
              </p>
            </div>
          </div>
        </div>
      </div>
    </footer>
  </body>
</html>
